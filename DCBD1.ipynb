{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **WEB SCRAPING**\n"
      ],
      "metadata": {
        "id": "5ZMW7Ydn5_5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IMPORTING LIBRARIES:"
      ],
      "metadata": {
        "id": "rCb7n2o0i3C2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T1qm2S5mzfWI"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import *\n",
        "import io, nltk, pathlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aJMQ-BJI0J58",
        "outputId": "e91eeb6c-9b2d-42db-df6a-cdd9bcd1ca87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\")) \n",
        "space_gained, space_input = 0, 0"
      ],
      "metadata": {
        "id": "2TmJ1Zua0Nvf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAIN CODE:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r71BsNebi-mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Welcome to Spazer\\n\")\n",
        "\n",
        "for x in range(10):\n",
        "    filename = str(x) + \".html\"\n",
        "    file = pathlib.Path('input/' + filename)\n",
        "\n",
        "    if (file.exists()):\n",
        "        print(\"Reading \" + filename)\n",
        "\n",
        "        f = open('input/' + filename, 'r', errors = \"ignore\")\n",
        "        contents = f.read()   \n",
        "        \n",
        "        #Extracting the html content from the html file\n",
        "        soup = BeautifulSoup(contents,'html.parser')   \n",
        "        \n",
        "        #Tokenizing every word of the file/Splitting a sentence into list of words\n",
        "        word_tokens = word_tokenize(soup.get_text()) \n",
        "\n",
        "        #Removing the stopwords from the file\n",
        "        filtered_words = [w for w in word_tokens if not w.lower() in stop_words] \n",
        "\n",
        "        #Converting list to string\n",
        "        list_to_str = \" \".join(map(str, filtered_words)) \n",
        "\n",
        "        #Giving the regular expression\n",
        "        tokenizer = RegexpTokenizer(r'\\w+') \n",
        "\n",
        "        #Matches all words with the regular expression\n",
        "        output_1 = tokenizer.tokenize(list_to_str) \n",
        "        output_2= []\n",
        "\n",
        "        #Extracting keywords_1\n",
        "        keywords_1 = ['address', 'Contact', 'contact', 'Address', 'Park', 'Sector', 'Plot', 'Road', 'Avenue', 'Floor', 'Building', 'Pin'] \n",
        "\n",
        "        for k in keywords_1:\n",
        "          for l in keywords_1:\n",
        "            if (k in output_1) and (l in output_1[(output_1.index(k)):(output_1.index(k)) + 60]) and (k != l) and (len(output_1[(output_1.index(k)):(output_1.index(l)) + 15]) > 5):\n",
        "              output_2 = output_2 + (output_1[(output_1.index(k)) - 10:(output_1.index(l)) + 10])\n",
        "              output_3 = ' '.join(words for words in output_2)\n",
        "              \n",
        "        output_4 = word_tokenize(output_3)\n",
        "\n",
        "        #Finding parts of speech of each word\n",
        "        output_5 = nltk.pos_tag(output_4) \n",
        "        output_6 = []\n",
        "\n",
        "        #Type of words to be removed from the output\n",
        "        keywords_2 = ['VB', 'VBP', 'VBD', 'MD', 'VBN', 'TO', 'DT', 'PRP', 'VBZ', 'VBG', 'NNS', 'RB', 'JJ'] \n",
        "\n",
        "        for l in output_5:\n",
        "          for k in keywords_2:\n",
        "            if l[1] == k: output_5.remove(l)\n",
        "\n",
        "        for l in output_5:\n",
        "          output_6 = output_6 + [l[0]]\n",
        "        output_7 = ' '.join(words for words in output_6)\n",
        "\n",
        "        print (\"Writing reduced \" + filename)\n",
        "        \n",
        "        fw = open('output/' + filename, \"w\")\n",
        "        fw.write(output_7)\n",
        "        fw.close()\n",
        "        f.close()\n",
        "        \n",
        "        space_input = space_input + len(contents)\n",
        "        space_gained = space_gained + len(contents) - len(output_7)        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_dxcrM850QqO",
        "outputId": "795fb54e-8aff-43a1-fc66-0d0fff2482f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Spazer\n",
            "\n",
            "Reading 0.html\n",
            "Writing reduced 0.html\n",
            "Reading 1.html\n",
            "Writing reduced 1.html\n",
            "Reading 2.html\n",
            "Writing reduced 2.html\n",
            "Reading 3.html\n",
            "Writing reduced 3.html\n",
            "Reading 4.html\n",
            "Writing reduced 4.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SPACE GAINED:"
      ],
      "metadata": {
        "id": "orLTqZPMjGAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Space Gained = \" + str(round(space_gained * 100 / space_input, 2)) + \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7xLHESSL0VVr",
        "outputId": "ca364621-de08-4de4-c905-05e08c97508d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Space Gained = 98.09%\n"
          ]
        }
      ]
    }
  ]
}